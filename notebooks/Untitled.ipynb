{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f0702bb-e2cd-4ea5-a030-672b8daad270",
   "metadata": {},
   "source": [
    "# Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "447f8937-f366-4398-90e4-e444bbad0c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/opt/app-root/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de26b9fd-b62c-46b1-934e-5ee755d3f70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.8.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"TensorFlow version: {}\".format(tf.__version__))\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Make numpy values easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f0f5d6-413c-411d-8f25-e590e36889be",
   "metadata": {},
   "source": [
    "# Load the data in memory\n",
    "\n",
    "For any small CSV vitals the simplest way to train a TensorFlow model on it is to load it into memory as a pandas Dataframe or a NumPy array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72864728-de00-4d99-8b39-886c4fb13193",
   "metadata": {},
   "source": [
    "## Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9eeb46-b0d5-4473-9487-b484fa6c03ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Pandas\n",
    "# read in only the columns for exploratory prediction\n",
    "#vitals = pd.read_csv('../data/raw/pat_vitals_labeled-dataSepsis.csv', usecols=['HR','Temp','Resp','isSepsis'])\n",
    "# print the data\n",
    "#vitals.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926beb14-1f10-4344-9f58-27f93dff9b58",
   "metadata": {},
   "source": [
    "## TFData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6eddb72b-6238-46d5-82a8-e08b51315f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using TFData\n",
    "vitals_file_path = '../data/raw/pat_vitals_labeled-dataSepsis.csv'\n",
    "# reads CSV files into a dataset\n",
    "vitals_csv_ds = tf.data.experimental.make_csv_dataset(\n",
    "    vitals_file_path,\n",
    "    batch_size=5, # Artificially small to make examples easier to show.\n",
    "    label_name='isSepsis', # data for this column is returned as a separate Tensor from the features dictionary\n",
    "    num_epochs=1, # int specifying the number of times this dataset is repeated\n",
    "    shuffle=True, #  bool that indicates whether the input should be shuffled\n",
    "    shuffle_buffer_size=100000,\n",
    "    shuffle_seed=42, # randomization seed to use for shuffling\n",
    "    ignore_errors=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "54911596-a944-460b-9ad6-33fe791ffd05",
   "metadata": {},
   "source": [
    "# helper function\n",
    "def csv_reader_dataset(\n",
    "        vitals_file_path,\n",
    "        batch_size=5, # Artificially small to make examples easier to show.\n",
    "        label_name='isSepsis', # data for this column is returned as a separate Tensor from the features dictionary\n",
    "        num_epochs=1,\n",
    "        shuffle=True, #  bool that indicates whether the input should be shuffled\n",
    "        shuffle_buffer_size=10000, # the number of feature batches to prefetch for performance improvement \n",
    "        shuffle_seed=42, # randomization seed to use for shuffling\n",
    "        ignore_errors=True) # If True, ignores errors with CSV file parsing, such as malformed data or empty lines, and moves on to the next valid CSV record\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads) # loading and preprocessing are multi-threaded for 100% utlization\n",
    "    return dataset.batch(batch_size).prefetch(1) # create a dataset that will be 1 batch ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79e6424d-9843-4d1a-90cd-5d8609ccb866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient_id          : [10884  3576 14519  2020  3352]\n",
      "record_date         : [b'' b'' b'' b'' b'']\n",
      "record_time         : [b'' b'' b'' b'' b'']\n",
      "HR                  : [88 60 86 79 71]\n",
      "O2Sat               : [b'96' b'100' b'94' b'96' b'100']\n",
      "Temp                : [35.9 35.8 36.2 42.  35.3]\n",
      "SBP                 : [b'95' b'96' b'97' b'143' b'124']\n",
      "MAP                 : [b'67' b'73.33' b'57' b'105' b'73']\n",
      "DBP                 : [b'57' b'NaN' b'42' b'81' b'53']\n",
      "Resp                : [27 16 15 18 18]\n",
      "EtCO2               : [b'NaN' b'NaN' b'NaN' b'NaN' b'NaN']\n",
      "\n",
      "label               : [1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "for batch, label in vitals_csv_ds.take(1):\n",
    "  for key, value in batch.items():\n",
    "    print(f\"{key:20s}: {value}\")\n",
    "  print()\n",
    "  print(f\"{'label':20s}: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bd97a05-7c68-402b-b0bd-9fb20a505720",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vitals_csv_ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-551e7ba97436>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# look at just a few items from the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvitals_csv_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vitals_csv_ds' is not defined"
     ]
    }
   ],
   "source": [
    "# look at just a few items from the dataset\n",
    "for item in vitals_csv_ds.take(1):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77fe0dc-6c58-48e2-9dde-d85de3f63591",
   "metadata": {},
   "source": [
    "# Preprocess\n",
    "\n",
    "The nominal task for this vitals is to predict if the patient is septic from the other measurements, so separate the features and labels for training:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d0c5e9-9012-47e5-9567-ce09d1b9123a",
   "metadata": {},
   "source": [
    "## Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "81ae388b-2117-4854-8812-ceaf46cb4f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vitals_features = vitals_csv_ds.copy()\n",
    "#vitals_labels = vitals_features.pop('isSepsis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61855539-dcc5-46dc-9e61-1a1091f011f3",
   "metadata": {},
   "source": [
    "For this vitals you will treat all features identically. Pack the features into a single NumPy array.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9712dc11-d3b9-4526-94f1-6da4ba5684a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vitals_features = np.array(vitals_features)\n",
    "#vitals_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d294fd13-2145-4198-994c-44fedb5c538d",
   "metadata": {},
   "source": [
    "It's good practice to normalize the inputs to your model. The Keras preprocessing layers provide a convenient way to build this normalization into your model. Note: Only use your training data to .adapt() preprocessing layers. Do not use your validation or test data.\n",
    "\n",
    "The layer will precompute the mean and variance of each column, and use these to normalize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e339ac1-b2b2-4a7d-8299-1d6ca06d38a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize = layers.Normalization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f4c7d7-6e29-45ad-ae8f-1e57d4809111",
   "metadata": {},
   "source": [
    "normalize.adapt(abalone_features)\n",
    "Then you use the Normalization.adapt() method to adapt the normalization layer to your data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcaba1c-b48f-4ec2-8818-a6994aabbbd5",
   "metadata": {},
   "source": [
    "## TF Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddee7054-cc7c-4467-adf1-e8e9fb90ff89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "987e36e4-ca34-4ca8-910a-c47514fa9fed",
   "metadata": {},
   "source": [
    "# Develop\n",
    "\n",
    "Next make a regression model predict the age. Since there is only a single input tensor, a keras.Sequential model is sufficient here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114f55aa-c54c-4c4c-b52b-0a37a2a7c08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_sepsis_model = tf.keras.Sequential([\n",
    "  normalize,\n",
    "  layers.Dense(64),\n",
    "  layers.Dense(1)\n",
    "])\n",
    "\n",
    "norm_sepsis_model.compile(loss = tf.losses.MeanSquaredError(),\n",
    "                           optimizer = tf.optimizers.Adam())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faa0201-8c4a-4c3d-9e43-40a95af095e6",
   "metadata": {},
   "source": [
    "# Train\n",
    "\n",
    "To train that model, pass the features and labels to Model.fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8633189d-5d6f-45c8-a453-b25ea5c5c8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_sepsis_model.fit(vitals_features, vitals_labels, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd922742-0895-45f1-9ca0-1ecfcf9131ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = norm_sepsis_model(vitals_features[:1].numpy()\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe2e1f7-0fb0-4132-bfde-10dfbe5d4af0",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33f1aaf-1245-471d-8f7b-65282a5d517c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
